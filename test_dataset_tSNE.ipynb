{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "# Save/Load as mat.\n",
    "from scipy.io import savemat, loadmat\n",
    "import h5py\n",
    "import mat73 # https://github.com/skjerns/mat7.3\n",
    "\n",
    "# Progress Bar\n",
    "from rich.progress import track\n",
    "from rich.progress import Progress\n",
    "\n",
    "# Other\n",
    "import argparse\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import scipy.signal as signal\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from plot import *\n",
    "\n",
    "# User-defined\n",
    "from mat2np_segment_all_subject import *\n",
    "from dsp_preprocess import *\n",
    "from dataset_parser import *\n",
    "from models import *\n",
    "from feature_extractor import *\n",
    "from set_args import *\n",
    "from train_test_process import *\n",
    "from vit import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/DB2/DB2_np/DB2_s_1/exercise_1/trial_1/ mean: 3.314649438834749e-05\n"
     ]
    }
   ],
   "source": [
    "idx_subject = 1\n",
    "idx_exercise = 1\n",
    "idx_trial = 1\n",
    "\n",
    "PATH_seg_np = \"Dataset/DB2/DB2_np/DB2_s_{}/exercise_{}/trial_{}/\".format(idx_subject,idx_exercise,idx_trial)\n",
    "fileNames = sorted([PATH_seg_np+i for i in os.listdir(PATH_seg_np)])\n",
    "# print(sorted(fileNames))\n",
    "mean_all_gesture = 0\n",
    "for fileName in fileNames:\n",
    "    gesture = fileName.split(\"/\")[-4:]\n",
    "    emg_sample_dataset, gesture_label_dataset = dataset_filter_normalize_segementation(fileName, 3, fs=2000, window_size = 400, window_step=200, num_channel=12, type_filter=\"none\", type_norm=\"none\")\n",
    "    emg_sample_dataset_LPF1, _ = dataset_filter_normalize_segementation(fileName, 1 ,fs=2000, window_size = 400, window_step=200, num_channel=12, type_filter=\"LPF_20_\", type_norm=\"mu_law_0.1\")\n",
    "    emg_sample_dataset_LPF3, _ = dataset_filter_normalize_segementation(fileName, 1 ,fs=2000, window_size = 400, window_step=200, num_channel=12, type_filter=\"LPF_20_\", type_norm=\"mu_law_1\")\n",
    "    emg_sample_dataset_LPF5, _ = dataset_filter_normalize_segementation(fileName, 1 ,fs=2000, window_size = 400, window_step=200, num_channel=12, type_filter=\"LPF_20_\", type_norm=\"mu_law_256\")\n",
    "    amp_mean = np.mean(np.mean(abs(emg_sample_dataset),axis=1), axis=0)\n",
    "    # print(amp_mean)\n",
    "    # print(f\"{gesture[-1]}, Mean: {np.mean(amp_mean)}, Max amplitude: {np.max(emg_sample_dataset)}\")\n",
    "    mean_all_gesture += np.mean(amp_mean)\n",
    "    # plt.figure(figsize=(6,2))\n",
    "    # plt.hist(abs(emg_sample_dataset.reshape(-1)))\n",
    "    \n",
    "    # print(emg_sample_dataset.shape)\n",
    "    # plt.figure(figsize=(6,2))\n",
    "    # plt.plot(emg_sample_dataset[0])\n",
    "    # plt.title(gesture)\n",
    "print(f\"{PATH_seg_np} mean: {mean_all_gesture/len(fileNames)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
